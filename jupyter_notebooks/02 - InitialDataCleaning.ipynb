{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd738370",
   "metadata": {},
   "source": [
    "# **Top Ten Player Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af16d5",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Answer business requirement one:\n",
    "  * The client wishes us to conduct an analysis of current elite-level golf tournament data \n",
    "    to determine which golfing skills (e.g., driving, approach play, chipping, and putting) \n",
    "    are most likely to result in a player reaching the top ten of a tournament. \n",
    "    They are specifically interested in learning which skill to focus on to help a player \n",
    "    improve from a 30th–11th place finish to a top-ten finish.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* inputs\\datasets\\raw\\ASA All PGA Raw Data - Tourn Level.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate code that answers business requirement 1 and can be used to build the StreamLit App.\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* Although more will be done later, a level of data cleaning is done in this notebook to sort the confusion in the data between 'pos' and 'finish' features discovered in the previous notebook. This was necessary at this stage to avoid analysing data with errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45114b96",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb9a2d",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cb307",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26ff58c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a74d12",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac0af2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba64db7c",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "272ba5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1507619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825b2b7",
   "metadata": {},
   "source": [
    "First, gain an overview of the data (bearing in mind missing fields and some data errors have been studied in the Data Collection phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e22ab98",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'inputs/datasets/raw/ASA All PGA Raw Data - Tourn Level.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33minputs/datasets/raw/ASA All PGA Raw Data - Tourn Level.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mData loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project-five-golf-data-analytics\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project-five-golf-data-analytics\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project-five-golf-data-analytics\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project-five-golf-data-analytics\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project-five-golf-data-analytics\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'inputs/datasets/raw/ASA All PGA Raw Data - Tourn Level.csv'"
     ]
    }
   ],
   "source": [
    "file_path = \"inputs/datasets/raw/ASA All PGA Raw Data - Tourn Level.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows × {df.shape[1]:,} columns\\n\")\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical features: 31\n",
      "['tournament id', 'player id', 'hole_par', 'strokes', 'hole_DKP', 'hole_FDP', 'hole_SDP', 'streak_DKP', 'streak_FDP', 'streak_SDP', 'n_rounds', 'made_cut', 'pos', 'finish_DKP', 'finish_FDP', 'finish_SDP', 'total_DKP', 'total_FDP', 'total_SDP', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'purse', 'season', 'no_cut', 'sg_putt', 'sg_arg', 'sg_app', 'sg_ott', 'sg_t2g', 'sg_total']\n",
      "\n",
      "Categorical features: 6\n",
      "['Player_initial_last', 'player', 'tournament name', 'course', 'date', 'Finish']\n"
     ]
    }
   ],
   "source": [
    "num_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "cat_features = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(\"\\nNumerical features:\", len(num_features))\n",
    "print(num_features)\n",
    "print(\"\\nCategorical features:\", len(cat_features))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d4afc",
   "metadata": {},
   "source": [
    "Check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e294e9",
   "metadata": {},
   "source": [
    "Before further data exploration, it is necessary to create a data frame of data that combines the pos and finish features (see data collection notebook) to check that this data will be suitable for the client's business requirements.\n",
    "To do this, first we need to make 'Finish' entirely numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    pos Finish finish_clean  finish_numeric\n",
      "0  32.0    T32           32            32.0\n",
      "1  18.0    T18           18            18.0\n",
      "2   NaN    CUT           CU             0.0\n",
      "3   NaN    CUT           CU             0.0\n",
      "4   NaN    CUT           CU             0.0\n",
      "5   NaN    CUT           CU             0.0\n",
      "6  26.0    T26           26            26.0\n",
      "7  26.0    T26           26            26.0\n",
      "8  67.0    T67           67            67.0\n",
      "9   NaN    CUT           CU             0.0\n"
     ]
    }
   ],
   "source": [
    "df_temp = df.copy()\n",
    "\n",
    "df_temp['finish_clean'] = df_temp['Finish'].astype(str).str.replace('T', '', regex=False)\n",
    "\n",
    "df_temp['finish_numeric'] = pd.to_numeric(df_temp['finish_clean'], errors='coerce')\n",
    "\n",
    "df_temp['finish_numeric'] = df_temp['finish_numeric'].fillna(0)\n",
    "\n",
    "print(df_temp[['pos', 'Finish', 'finish_clean', 'finish_numeric']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34533376",
   "metadata": {},
   "source": [
    "Next, check for discrepencies between finish_numeric and pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b57d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where finish_numeric and pos differ: 5505\n",
      "      pos Finish  finish_numeric\n",
      "94   18.0    NaN             0.0\n",
      "324  77.0    NaN             0.0\n",
      "325   8.0    NaN             0.0\n",
      "394  81.0    NaN             0.0\n",
      "534  41.0    NaN             0.0\n",
      "601   5.0    NaN             0.0\n",
      "677  33.0    NaN             0.0\n",
      "798  10.0    NaN             0.0\n",
      "800  21.0    NaN             0.0\n",
      "927  14.0    NaN             0.0\n"
     ]
    }
   ],
   "source": [
    "df_temp['pos_differs'] = df_temp['finish_numeric'] != df_temp['pos']\n",
    "num_differences = df_temp['pos_differs'].sum()\n",
    "print(f\"Number of rows where finish_numeric and pos differ: {num_differences}\")\n",
    "print(df_temp[df_temp['pos_differs']][['pos', 'Finish', 'finish_numeric']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fad5c9",
   "metadata": {},
   "source": [
    "Turn everything that is non numeric in 'pos' to a 0 (to indicate a bad finish in the tournament)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fadaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 8.0, 0.0, 63.0, 32.0, 3.0, 12.0, 15.0, 32.0, 11.0, 29.0, 0.0, 0.0, 0.0, 69.0, 29.0, 54.0, 0.0, 36.0]\n"
     ]
    }
   ],
   "source": [
    "df_temp['pos'] = df_temp['pos'].fillna(0)\n",
    "print(df_temp['pos'].sample(20, random_state=42).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e1618",
   "metadata": {},
   "source": [
    "Now check of any discrepencies between finish_numeric and pos again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b5d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where finish_numeric and pos differ: 5505\n",
      "      pos Finish  finish_numeric\n",
      "94   18.0    NaN             0.0\n",
      "324  77.0    NaN             0.0\n",
      "325   8.0    NaN             0.0\n",
      "394  81.0    NaN             0.0\n",
      "534  41.0    NaN             0.0\n",
      "601   5.0    NaN             0.0\n",
      "677  33.0    NaN             0.0\n",
      "798  10.0    NaN             0.0\n",
      "800  21.0    NaN             0.0\n",
      "927  14.0    NaN             0.0\n"
     ]
    }
   ],
   "source": [
    "df_temp['pos_differs'] = df_temp['finish_numeric'] != df_temp['pos']\n",
    "num_differences = df_temp['pos_differs'].sum()\n",
    "print(f\"Number of rows where finish_numeric and pos differ: {num_differences}\")\n",
    "print(df_temp[df_temp['pos_differs']][['pos', 'Finish', 'finish_numeric']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06f8eb",
   "metadata": {},
   "source": [
    "From manual checks during the Data Collection phase, we believe pos to be the more reliable field. However, in some cases finish_numeric will have a result in the top ten that is accurate and pos will be inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pos Finish  finish_numeric\n",
      "2771    0.0     T8             8.0\n",
      "3629    0.0      3             3.0\n",
      "4211    0.0     T6             6.0\n",
      "4434    0.0     T5             5.0\n",
      "8676    0.0      4             4.0\n",
      "8677    0.0     T2             2.0\n",
      "8685    0.0     T8             8.0\n",
      "8729    0.0     T5             5.0\n",
      "8737    0.0     T8             8.0\n",
      "8761    0.0      1             1.0\n",
      "8763    0.0     T8             8.0\n",
      "8765    0.0     T2             2.0\n",
      "8769    0.0     T8             8.0\n",
      "8772    0.0     T5             5.0\n",
      "8792    0.0     T8             8.0\n",
      "8811    0.0     T5             5.0\n",
      "8836    0.0     T6             6.0\n",
      "8838    0.0     T6             6.0\n",
      "8852    0.0     T6             6.0\n",
      "8862    0.0     T4             4.0\n",
      "8871    0.0      3             3.0\n",
      "8877    0.0     T4             4.0\n",
      "8887    0.0     T6             6.0\n",
      "8907    0.0      2             2.0\n",
      "8923    0.0      1             1.0\n",
      "8934    0.0     T6             6.0\n",
      "8940    0.0     T6             6.0\n",
      "18524   0.0     T7             7.0\n",
      "24437   0.0      2             2.0\n",
      "31451   2.0     T4             4.0\n",
      "31495  26.0     T4             4.0\n",
      "31506   2.0     T7             7.0\n",
      "31512   0.0    T10            10.0\n",
      "31513  54.0    T10            10.0\n",
      "31547  10.0     T7             7.0\n",
      "35527  14.0      3             3.0\n",
      "35544   2.0     T9             9.0\n",
      "35549  30.0     T6             6.0\n",
      "35608   8.0     T6             6.0\n",
      "35626   0.0     T4             4.0\n",
      "36409   0.0     T4             4.0\n",
      "\n",
      "Number of mismatches: 41\n"
     ]
    }
   ],
   "source": [
    "top_ten_mismatches = df_temp[\n",
    "    (df_temp['finish_numeric'].between(1, 10, inclusive='both')) &\n",
    "    (df_temp['finish_numeric'] != df_temp['pos'])\n",
    "]\n",
    "print(top_ten_mismatches[['pos', 'Finish', 'finish_numeric']])\n",
    "print(f\"\\nNumber of mismatches: {len(top_ten_mismatches)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42258d2",
   "metadata": {},
   "source": [
    "We now need to create a new feature that uses the pos value (apart from these 41 which will use finish) cases called true_pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['true_pos'] = np.where(\n",
    "    (df_temp['finish_numeric'].between(1, 10, inclusive='both')) &\n",
    "    (df_temp['finish_numeric'] != df_temp['pos']),\n",
    "    df_temp['finish_numeric'],\n",
    "    df_temp['pos']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dbcc0",
   "metadata": {},
   "source": [
    "Print one occurence where there was an issue to check for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997582c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos         0.0\n",
      "true_pos    8.0\n",
      "Name: 2771, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_temp.loc[2771, ['pos', 'true_pos']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b0f3a",
   "metadata": {},
   "source": [
    "Finally, use the true_pos value to create a new feature called top_ten, whereby 0 = not in the top ten and 1 = in the top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c6bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_ten\n",
      "0    33116\n",
      "1     3748\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_temp['top_ten'] = np.where(df_temp['true_pos'].between(1, 10, inclusive='both'), 1, 0)\n",
    "print(df_temp['top_ten'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648794b8",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps ##\n",
    "\n",
    "- Further data cleaning is necessary to remove unwanted features and consider what to do with fields with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b157f",
   "metadata": {},
   "source": [
    "## Push files to Repo\n",
    "It will be time-efficient to push the df_temp dataframe to the repo in preparation for further data cleaning in the next phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b076cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created at: outputs/data/interim\n",
      "Data saved to: outputs/data/interim\\cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"outputs/data/interim\"\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    raise FileExistsError(f\"The folder '{output_path}' already exists. Please remove or rename it before continuing.\")\n",
    "else:\n",
    "    os.makedirs(output_path)\n",
    "    print(f\"Folder created at: {output_path}\")\n",
    "\n",
    "file_path = os.path.join(output_path, \"cleaned_data.csv\")\n",
    "df_temp.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
